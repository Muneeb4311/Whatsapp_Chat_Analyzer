# -*- coding: utf-8 -*-
"""Whatsapp_Chat_Analyzer And Sentiment_Analyzer

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VnXsvwdHItOil1bQLuWg0XGSaGsZZCdX

**Whatsapp_Chat_Analysis Part:**

Required Libraries:
"""

!pip install gradio
!pip install regex
!pip install emoji
!pip install plotly

import re
import regex
import pandas as pd
import numpy as np
from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
import emoji
import plotly.express as px
import gradio as gr

from textblob import TextBlob

"""Validates date and time format:"""

def date_time(s):
    pattern = '^([0-9]+)(\/)([0-9]+)(\/)([0-9]+), ([0-9]+):([0-9]+)[ ]?(AM|PM|am|pm)? -'
    result = regex.match(pattern, s)
    if result:
        return True
    return False
# Checks for author presence
def find_author(s):
    s = s.split(":")
    if len(s)==2:
        return True
    else:
        return False
   # Extracts date, time, author, and message.
def getDatapoint(line):
    splitline = line.split(' - ')
    dateTime = splitline[0]
    date, time = dateTime.split(", ")
    message = " ".join(splitline[1:])
    if find_author(message):
        splitmessage = message.split(": ")
        author = splitmessage[0]
        message = " ".join(splitmessage[1:])
    else:
        author= None
    return date, time, author, message

"""Extracts and organizes chat messages from a file into a structured format:"""

data = []
conversation = '/content/Whatsapp_chat.txt'
with open(conversation, encoding="utf-8") as fp:
    fp.readline()
    messageBuffer = []
    date, time, author = None, None, None
    while True:
        line = fp.readline()
        if not line:
            break
        line = line.strip()
        if date_time(line):
            if len(messageBuffer) > 0:
                data.append([date, time, author, ' '.join(messageBuffer)])
            messageBuffer.clear()
            date, time, author, message = getDatapoint(line)
            messageBuffer.append(message)
        else:
            messageBuffer.append(line)

"""Creates a DataFrame from chat data, converts dates, and displays summary information:"""

df = pd.DataFrame(data, columns=["Date", 'Time', 'Author', 'Message'])
df['Date'] = pd.to_datetime(df['Date'])
print(df.tail(20))
print(df.info())
print(df.Author.unique())

"""Counts and prints the total number of messages in the DataFrame:"""

total_messages = df.shape[0]
print(total_messages)

"""Counts and prints the number of media messages in the DataFrame:"""

media_messages = df[df["Message"]=='<Media omitted>'].shape[0]
print(media_messages)

"""Counts and summarizes total messages, media shared, and links in the chat data:"""

URLPATTERN = r'(https?://\S+)'
df['urlcount'] = df.Message.apply(lambda x: regex.findall(URLPATTERN, x)).str.len()
links = np.sum(df.urlcount)

print("Chats between all members")
print("Total Messages: ", total_messages)
print("Number of Media Shared: ", media_messages)
print("Number of Links Shared", links)

"""Analyzes WhatsApp chat data by calculating message statistics, filtering out media messages, and summarizing user activity, including message counts, average words, emojis, and links sent:"""

# Read the data from a text file
file_path = '/content/Whatsapp_chat.txt'

# Load the data into a DataFrame
data = []
with open(file_path, 'r') as file:
    for line in file:
        parts = line.strip().split(': ', 1)  # Split into Author and Message
        if len(parts) == 2:
            author, message = parts
            data.append({'Author': author, 'Message': message})

df = pd.DataFrame(data)

# Filtering out media messages
media_messages_df = df[df['Message'] == '<Media omitted>']
messages_df = df.drop(media_messages_df.index)

# Calculating letter and word counts
messages_df['Letter_Count'] = messages_df['Message'].apply(len)
messages_df['Word_Count'] = messages_df['Message'].apply(lambda s: len(s.split()))
messages_df['MessageCount'] = 1

# Users list (You can modify this as needed)
users = messages_df['Author'].unique()  # Get unique users from the DataFrame

# Analyzing messages for each user
for user in users:
    # Filtering messages for the specific user
    req_df = messages_df[messages_df['Author'] == user]

    # Displaying stats for the user
    print(f'Stats of {user} -')
    print('Messages Sent:', req_df.shape[0])

    if req_df.shape[0] > 0:  # Avoid division by zero
        words_per_message = np.sum(req_df['Word_Count']) / req_df.shape[0]
        print('Average Words per message:', words_per_message)
    else:
        print('Average Words per message: 0')

    # Counting media messages
    media_count = media_messages_df[media_messages_df['Author'] == user].shape[0]
    print('Media Messages Sent:', media_count)

    # Counting emojis and links (assuming emojis and URLs are counted elsewhere)
    req_df['emoji'] = req_df['Message'].str.count(r'[\U0001F600-\U0001F64F]')  # Example emoji regex
    req_df['urlcount'] = req_df['Message'].str.count(r'http[s]?://\S+')  # Example URL regex

    emojis = req_df['emoji'].sum()
    print('Emojis Sent:', emojis)

    links = req_df['urlcount'].sum()
    print('Links Sent:', links)

    print('---')  # Separator for clarity

"""Analyzes WhatsApp chat data to extract and count emoji faces, then visualizes the distribution with a pie chart:"""

# Define the file path to your group chat data
file_path = '/content/Whatsapp_chat.txt'  # Update this to your actual file path

# Load the data
with open(file_path, 'r', encoding='utf-8') as file:
    messages = file.readlines()

# Initialize a list to hold all emoji faces
all_emoji_faces = []

# Define a regex pattern for common emoji faces (modify as needed)
emoji_face_pattern = re.compile(r'[\U0001F600-\U0001F64F]')  # This range covers common emoji faces

# Extract emoji faces from messages
for message in messages:
    # Find all emoji faces in the message
    faces = emoji_face_pattern.findall(message)
    all_emoji_faces.extend(faces)

# Count occurrences of each emoji face
emoji_face_dict = dict(Counter(all_emoji_faces))
emoji_face_dict = sorted(emoji_face_dict.items(), key=lambda x: x[1], reverse=True)

# Print the emoji face counts
for emoji, count in emoji_face_dict:
    print(f"{emoji}: {count}")

# Create a DataFrame for the emoji faces and their counts
emoji_face_df = pd.DataFrame(emoji_face_dict, columns=['emoji', 'count'])

# Create a pie chart of the emoji face counts
fig = px.pie(emoji_face_df, values='count', names='emoji', title='Emoji Face Usage in Group Chats')
fig.update_traces(textposition='inside', textinfo='percent+label')
fig.show()

"""Creates a word cloud from all messages, visualizing word frequency while excluding stopwords:"""

text = " ".join(review for review in messages_df.Message)
print ("There are {} words in all the messages.".format(len(text)))
stopwords = set(STOPWORDS)
# Generate a word cloud image
wordcloud = WordCloud(stopwords=stopwords, background_color="white").generate(text)
# Display the generated image:
# the matplotlib way:
plt.figure( figsize=(10,5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

"""Generates and displays individual word clouds for each author in the WhatsApp chat data, visualizing their unique word usage:"""

# Read the data from a text file
file_path = '/content/Whatsapp_chat.txt'

# Load the data into a DataFrame
data = []
with open(file_path, 'r', encoding='utf-8') as file:
    for line in file:
        # Use regex to capture the timestamp, author, and message
        match = re.match(r'^\[(.*?)] (.*?): (.*)$', line.strip())
        if match:
            date_time, author, message = match.groups()
            data.append({'Author': author, 'Message': message})

# Check if any data was loaded
if not data:
    print("No valid messages found in the file.")
else:
    messages_df = pd.DataFrame(data)

    # Get unique authors from the messages
    authors = messages_df['Author'].unique()

    # Generate word clouds for each author
    for author in authors:
        # Filter messages for the specific author
        dummy_df = messages_df[messages_df['Author'] == author]

        if dummy_df.empty:
            print(f"No messages found for {author}.")
            continue

        # Combine all messages into a single text
        text = " ".join(review for review in dummy_df.Message)

        # Set of stopwords
        stopwords = set(STOPWORDS)

        # Generate a word cloud image
        print('Generating word cloud for:', author)
        wordcloud = WordCloud(stopwords=stopwords, background_color="white").generate(text)

        # Display the generated image
        plt.figure(figsize=(10, 5))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis("off")
        plt.title(f'Word Cloud for {author}')
        plt.show()

"""Sentiments_Analysis Part

Required Libraries:
"""

import re
import pandas as pd
import numpy as np
from nltk.sentiment import SentimentIntensityAnalyzer
from collections import Counter
import matplotlib.pyplot as plt
from PIL import Image
import nltk
nltk.download('vader_lexicon')

from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator

"""Extract Time:"""

def date_time(s):
    pattern = '^([0-9]+)(\/)([0-9]+)(\/)([0-9]+), ([0-9]+):([0-9]+)[ ]?(AM|PM|am|pm)? -'
    result = re.match(pattern, s)
    return bool(result)

"""Find Authors or Contacts:"""

def find_author(s):
    s = s.split(":")
    return len(s) == 2

"""Finding Messages:"""

def getDatapoint(line):
    splitline = line.split(' - ')
    dateTime = splitline[0]
    date, time = dateTime.split(", ")
    message = " ".join(splitline[1:])
    if find_author(message):
        splitmessage = message.split(": ")
        author = splitmessage[0]
        message = " ".join(splitmessage[1:])
    else:
        author = None
    return date, time, author, message

"""Reading and parsing data:"""

data = []
conversation = 'Whatsapp_chat.txt'
with open(conversation, encoding="utf-8") as fp:
    fp.readline()
    messageBuffer = []
    date, time, author = None, None, None
    while True:
        line = fp.readline()
        if not line:
            break
        line = line.strip()
        if date_time(line):
            if messageBuffer:
                data.append([date, time, author, ' '.join(messageBuffer)])
            messageBuffer.clear()
            date, time, author, message = getDatapoint(line)
            messageBuffer.append(message)
        else:
            messageBuffer.append(line)

"""Creating DataFrame:"""

df = pd.DataFrame(data, columns=["Date", 'Time', 'Author', 'Message'])
df['Date'] = pd.to_datetime(df['Date'])
data = df.dropna()

"""Sentiment Analysis:"""

sid = SentimentIntensityAnalyzer()
data["Positive"] = data["Message"].apply(lambda x: sid.polarity_scores(x)["pos"])
data["Negative"] = data["Message"].apply(lambda x: sid.polarity_scores(x)["neg"])
data["Neutral"] = data["Message"].apply(lambda x: sid.polarity_scores(x)["neu"])

"""Display Data with Sentiment Scores:"""

print(data[['Date', 'Author', 'Message', 'Positive', 'Negative', 'Neutral']].head(10))

"""Summing and calculating average sentiment scores:"""

x = data["Positive"].mean()
y = data["Negative"].mean()
z = data["Neutral"].mean()

"""Determining Overall Sentiment:"""

def sentiment_score(avg_pos, avg_neg, avg_neu):
    if (avg_pos > avg_neg) and (avg_pos > avg_neu):
        return f"Overall Sentiment: Positive ðŸ˜Š (Positive: {avg_pos:.2f}, Negative: {avg_neg:.2f}, Neutral: {avg_neu:.2f})"
    elif (avg_neg > avg_pos) and (avg_neg > avg_neu):
        return f"Overall Sentiment: Negative ðŸ˜  (Positive: {avg_pos:.2f}, Negative: {avg_neg:.2f}, Neutral: {avg_neu:.2f})"
    else:
        return f"Overall Sentiment: Neutral ðŸ™‚ (Positive: {avg_pos:.2f}, Negative: {avg_neg:.2f}, Neutral: {avg_neu:.2f})"

"""Print the final sentiment score:"""

print(sentiment_score(x, y, z))

def analyze_chat(file):
    with open(file.name, 'r', encoding='utf-8') as f:
        chat_data = f.read()

    # Split messages based on line breaks
    messages = chat_data.split('\n')

    total_messages = len(messages)
    total_links = sum(bool(re.search(r'http[s]?://', message)) for message in messages)
    total_media_files = sum('media' in message.lower() for message in messages)

    # Sentiment analysis
    positive_count = 0
    negative_count = 0
    neutral_count = 0

    for message in messages:
        analysis = TextBlob(message)
        # Classify the message sentiment
        if analysis.sentiment.polarity > 0:
            positive_count += 1
        elif analysis.sentiment.polarity < 0:
            negative_count += 1
        else:
            neutral_count += 1

    return (total_messages, total_links, total_media_files,
            positive_count, negative_count, neutral_count)

# Create a Gradio interface
interface = gr.Interface(
    fn=analyze_chat,
    inputs=gr.File(label="Upload WhatsApp Chat (text file)"),
    outputs=[
        gr.Textbox(label="Total Number of Messages"),
        gr.Textbox(label="Total Links Shared"),
        gr.Textbox(label="Total Media Files Shared"),
        gr.Textbox(label="Total Positive Messages"),
        gr.Textbox(label="Total Negative Messages"),
        gr.Textbox(label="Total Neutral Messages"),
    ],
    title="WhatsApp Chat Analyzer",
    description="Upload a WhatsApp chat text file to analyze the total messages, links shared, media files shared, and sentiment."
)

# Launch the interface
interface.launch()